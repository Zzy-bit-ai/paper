# -*- coding: utf-8 -*-
import os
import sys
import pandas as pd
import numpy as np
import random
import time
import pickle
import argparse
import requests
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from dotenv import load_dotenv

# ==================== ç¯å¢ƒåŠ è½½ ====================
load_dotenv()
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")

# ==================== å‚æ•°é…ç½® ====================
parser = argparse.ArgumentParser(description="KATE æƒ…æ„Ÿåˆ†ç±»åœ¨çº¿è¯„ä¼° - ä¼˜åŒ–ç‰ˆ")
parser.add_argument('--data_dir', type=str, default='./dataset', help='æ•°æ®é›†æ‰€åœ¨ç›®å½•')
parser.add_argument('--train_file', type=str, default='SST-2_train.tsv')
parser.add_argument('--dev_file', type=str, default='IMDB_dev.tsv')
parser.add_argument('--knn_file', type=str, default=None, help='kNN ç´¢å¼•æ–‡ä»¶è·¯å¾„')
parser.add_argument('--k', type=int, default=8, help='ç¤ºä¾‹æ•°é‡')
parser.add_argument('--sample_size', type=int, default=2000, help='è¯„ä¼°æ ·æœ¬æ€»æ•°')
parser.add_argument('--epochs', type=int, default=3, help='é‡å¤è¿è¡Œæ¬¡æ•°')
parser.add_argument('--mode', type=str, default='kate', choices=['random', 'kate'])
parser.add_argument('--model', type=str, default='xiaomi/mimo-v2-flash:free')
parser.add_argument('--max_workers', type=int, default=10, help='API å¹¶å‘æ•°')
args = parser.parse_args()

# ==================== åŠŸèƒ½ç»„ä»¶ ====================

def call_mimo_api(prompt):
    """API è°ƒç”¨ï¼šå¢åŠ æ˜ç¡®çš„è¶…æ—¶å’Œé‡è¯•é€»è¾‘"""
    url = "https://openrouter.ai/api/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": args.model,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.0,
        "max_tokens": 15
    }
    
    for attempt in range(3):
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=45)
            if response.status_code == 200:
                content = response.json()['choices'][0]['message']['content']
                return content.strip().lower()
            else:
                print(f"âš ï¸ API Error {response.status_code}: {response.text}")
        except Exception as e:
            if attempt < 2: time.sleep(2 * (attempt + 1))
    return "[API_ERROR]"

def is_correct(pred, gold_label):
    """æ ‡ç­¾åˆ¤æ–­ï¼šæ›´åŠ ä¸¥è°¨çš„é€»è¾‘"""
    pred = pred.lower()
    # è¿‡æ»¤æ‰å¹²æ‰°æ€§å‰ç¼€ï¼Œå¦‚ "the sentiment is positive"
    if gold_label == 1: # Positive
        return "positive" in pred and "not positive" not in pred
    else: # Negative
        return "negative" in pred and "not negative" not in pred

def normalize_data(df):
    """æ•°æ®æ ‡å‡†åŒ–"""
    label_col = 'label' if 'label' in df.columns else 'Sentiment'
    df['std_label'] = df[label_col].apply(lambda x: 1 if str(x).lower() in ['1', 'positive', 'pos'] else 0)
    for col in ['sentence', 'text', 'Sentence', 'Text']:
        if col in df.columns:
            df = df.rename(columns={col: 'std_text'})
            break
    return df

# ==================== ä¸»ç¨‹åºé€»è¾‘ ====================

def main():
    if not OPENROUTER_API_KEY:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ° API KEYï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶ã€‚")
        return

    # 1. åŠ è½½æ•°æ®
    try:
        train_df = normalize_data(pd.read_csv(os.path.join(args.data_dir, args.train_file), sep='\t'))
        dev_full = normalize_data(pd.read_csv(os.path.join(args.data_dir, args.dev_file), sep='\t'))
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸã€‚è®­ç»ƒé›†: {len(train_df)}, éªŒè¯é›†: {len(dev_full)}")
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
        return

    # 2. åŠ è½½ KNN
    knn_indices = None
    if args.mode == 'kate':
        try:
            with open(args.knn_file, "rb") as f:
                knn_data = pickle.load(f)
            knn_indices = knn_data.get("kNN_dev_train") if knn_data.get("kNN_dev_train") is not None else knn_data.get("indices")
            knn_indices = np.array(knn_indices)
            print(f"âœ… kNN ç´¢å¼•åŠ è½½æˆåŠŸï¼ŒShape: {knn_indices.shape}")
        except Exception as e:
            print(f"âŒ kNN æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            return

    results_across_epochs = []

    # 3. Epoch å¾ªç¯
    for ep in range(args.epochs):
        print(f"\n--- Epoch {ep+1}/{args.epochs} ({args.mode.upper()}) ---")
        
        # æ ·æœ¬æ´—ç‰Œä¸æŠ½æ ·ï¼ˆé’ˆå¯¹ Random æ¨¡å¼å¢åŠ å®éªŒå¤šæ ·æ€§ï¼‰
        if args.mode == 'random':
            current_dev = dev_full.sample(n=min(args.sample_size, len(dev_full)), random_state=ep).reset_index()
        else:
            current_dev = dev_full.head(args.sample_size).reset_index()

        prompts = []
        gold_labels = []

        for i in range(len(current_dev)):
            # è·å–åŸå§‹ç´¢å¼•ä»¥å¯¹é½ kNN
            original_idx = current_dev.iloc[i]['index']
            
            if args.mode == 'kate':
                idxs = knn_indices[original_idx][:args.k]
            else:
                random.seed(ep * 1000 + i)
                idxs = random.sample(range(len(train_df)), args.k)
            
            # æ„å»º Prompt
            p = "Instruction: Classify the following movie review as positive or negative.\n\n"
            for tidx in idxs:
                row = train_df.iloc[tidx]
                lbl = "positive" if row['std_label'] == 1 else "negative"
                p += f"Review: {row['std_text']}\nSentiment: {lbl}\n\n"
            
            p += f"Review: {current_dev.iloc[i]['std_text']}\nSentiment:"
            prompts.append(p)
            gold_labels.append(current_dev.iloc[i]['std_label'])

        # 4. å¹¶å‘æ¨ç†ä¸ç»“æœå¯¹é½
        predictions = [None] * len(prompts)
        with ThreadPoolExecutor(max_workers=args.max_workers) as executor:
            # ä½¿ç”¨å­—å…¸å»ºç«‹ future ä¸åŸå§‹ç´¢å¼•çš„æ˜ å°„
            future_to_idx = {executor.submit(call_mimo_api, p): i for i, p in enumerate(prompts)}
            
            for future in tqdm(as_completed(future_to_idx), total=len(future_to_idx), desc="Inference"):
                idx = future_to_idx[future]
                try:
                    predictions[idx] = future.result()
                except Exception as e:
                    predictions[idx] = "[ERROR]"

        # 5. è®¡ç®—åˆ†æ•°
        correct = sum(1 for p, g in zip(predictions, gold_labels) if is_correct(p, g))
        acc = (correct / len(gold_labels)) * 100
        results_across_epochs.append(acc)
        print(f"â­ Epoch {ep+1} Accuracy: {acc:.2f}%")

    # 6. æœ€ç»ˆç»Ÿè®¡
    mean_acc = np.mean(results_across_epochs)
    std_acc = np.std(results_across_epochs)
    
    print(f"\n{'='*50}")
    print(f"ğŸ† Final Mean Accuracy: {mean_acc:.2f}% Â± {std_acc:.2f}")
    print(f"Details: {results_across_epochs}")
    print(f"{'='*50}")

    # ä¿å­˜
    os.makedirs("result", exist_ok=True)
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"result/Sent_{args.mode}_k{args.k}_{ts}.txt"
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(f"Model: {args.model}\nMode: {args.mode}\nK: {args.k}\n")
        f.write(f"Mean: {mean_acc:.4f}\nStd: {std_acc:.4f}\nAll: {results_across_epochs}")

if __name__ == "__main__":
    main()