# train_ranker_v3_final.py
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from tqdm import tqdm
import random
import argparse
import re
import string
import hashlib
import sqlite3
from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer, util
from transformers import AutoModelForCausalLM, AutoTokenizer

# ================= é…ç½® =================
REAL_DATA_PATH = r"D:\py_code\paper\KATEGPT3-main\KATEGPT3-main\inference\dataset\trivia_qa_train_78785_dev_full_train.tsv"
SAVE_DIR = "./processed_data_v3"
EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2'
GPT2_MODEL_NAME = 'gpt2'
DB_PATH = "features_cache_v3.db"

# ================= å‚æ•° =================
parser = argparse.ArgumentParser()
parser.add_argument('--train_file', type=str, default=REAL_DATA_PATH)
parser.add_argument('--epochs', type=int, default=5)
parser.add_argument('--batch_size', type=int, default=64)
parser.add_argument('--lr', type=float, default=1e-3)
parser.add_argument('--k_candidates', type=int, default=50) # æ‰©å¤§å€™é€‰æ± 
parser.add_argument('--neg_ratio', type=int, default=5)
args = parser.parse_args()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ================= å·¥å…·å‡½æ•° =================
def normalize_answer(s):
    def remove_articles(text): return re.sub(r'\b(a|an|the)\b', ' ', text)
    def white_space_fix(text): return ' '.join(text.split())
    def remove_punc(text): return ''.join(ch for ch in text if ch not in string.punctuation)
    def lower(text): return text.lower()
    return white_space_fix(remove_articles(remove_punc(lower(str(s)))))

def is_positive_sample(cand_ans, gold_ans):
    """[ä¿®å¤ #1] åŒå‘åŒ…å«æˆ–ç›¸ç­‰"""
    nc = normalize_answer(cand_ans)
    ng = normalize_answer(gold_ans)
    if not nc or not ng: return False
    return (nc == ng) or (nc in ng) or (ng in nc)

def get_md5(text):
    return hashlib.md5(text.encode('utf-8')).hexdigest()

def smart_read_csv(file_path):
    print(f"ğŸ“– è¯»å–æ–‡ä»¶: {file_path}")
    try:
        df = pd.read_csv(file_path, sep='\t', quoting=3, on_bad_lines='skip').fillna("")
        if len(df.columns) < 2: df = pd.read_csv(file_path, sep=',', quoting=3, on_bad_lines='skip').fillna("")
    except:
        df = pd.read_csv(file_path, sep=None, engine='python', quoting=3, on_bad_lines='skip').fillna("")
    
    col_map = {}
    for c in df.columns:
        if str(c).lower().strip() in ['q', 'question']: col_map[c] = 'q'
        if str(c).lower().strip() in ['a', 'answer']: col_map[c] = 'a'
    df.rename(columns=col_map, inplace=True)
    if 'q' not in df.columns: df['q'] = df.iloc[:,0]
    if 'a' not in df.columns: df['a'] = df.iloc[:,1]
    return df

# ================= PPL è®¡ç®— (å¸¦ç¼“å­˜) =================
class PPLEngine:
    def __init__(self, model_path, db_path):
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        if self.tokenizer.pad_token is None: self.tokenizer.pad_token = self.tokenizer.eos_token
        self.model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16).to(device)
        self.model.eval()
        self.conn = sqlite3.connect(db_path)
        self.cursor = self.conn.cursor()
        self.cursor.execute('CREATE TABLE IF NOT EXISTS ppl (hash TEXT PRIMARY KEY, val REAL)')
        self.conn.commit()
    
    def get_ppl_batch(self, prompts, hashes):
        # 1. æŸ¥ç¼“å­˜
        results = {}
        missing_indices = []
        missing_prompts = []
        
        # æ‰¹é‡æŸ¥åº“
        if not hashes: return []
        
        # SQLite é™åˆ¶ï¼Œåˆ†æ‰¹æŸ¥
        CHUNK = 900
        for i in range(0, len(hashes), CHUNK):
            sub_h = hashes[i:i+CHUNK]
            p_str = ','.join(['?']*len(sub_h))
            self.cursor.execute(f"SELECT hash, val FROM ppl WHERE hash IN ({p_str})", sub_h)
            results.update({r[0]: r[1] for r in self.cursor.fetchall()})
            
        final_ppls = []
        for i, h in enumerate(hashes):
            if h in results:
                final_ppls.append(results[h])
            else:
                final_ppls.append(None)
                missing_indices.append(i)
                missing_prompts.append(prompts[i])
        
        # 2. è®¡ç®—ç¼ºå¤±
        if missing_prompts:
            # åˆ†æ‰¹æ¨ç†
            BS = 32
            new_vals = []
            for j in range(0, len(missing_prompts), BS):
                batch_p = missing_prompts[j:j+BS]
                inputs = self.tokenizer(batch_p, return_tensors="pt", truncation=True, max_length=512, padding=True).to(device)
                with torch.no_grad():
                    outputs = self.model(**inputs, labels=inputs["input_ids"])
                
                # Shift logits
                shift_logits = outputs.logits[..., :-1, :].contiguous()
                shift_labels = inputs["input_ids"][..., 1:].contiguous()
                loss_fct = nn.CrossEntropyLoss(reduction='none')
                loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
                loss = loss.view(shift_labels.size())
                
                # Mean per sequence
                lengths = (shift_labels != self.tokenizer.pad_token_id).sum(dim=1).float().clamp(min=1.0)
                seq_loss = loss.sum(dim=1) / lengths
                batch_ppl = torch.exp(seq_loss.clamp(max=15.0)).cpu().numpy().tolist() # max=15é˜²æ­¢æº¢å‡º
                new_vals.extend(batch_ppl)
            
            # 3. å†™å…¥ç¼“å­˜å¹¶å¡«å›
            rows = []
            for k, val in enumerate(new_vals):
                origin_idx = missing_indices[k]
                final_ppls[origin_idx] = val
                rows.append((hashes[origin_idx], val))
            
            self.cursor.executemany("INSERT OR IGNORE INTO ppl VALUES (?, ?)", rows)
            self.conn.commit()
            
        return final_ppls

    def close(self): self.conn.close()

# ================= ç‰¹å¾å¤„ç† =================
def build_dataset(query_df, corpus_df, bm25, sbert, ppl_engine, name):
    os.makedirs(SAVE_DIR, exist_ok=True)
    save_path = os.path.join(SAVE_DIR, f"{name}_v3.pt")
    
    if os.path.exists(save_path):
        print(f"ğŸ“¦ å‘ç°ç¼“å­˜ {name}, ç›´æ¥åŠ è½½...")
        return torch.load(save_path)

    print(f"âš™ï¸ å¼€å§‹å¤„ç† {name} (3-Feature Mode)...")
    
    # 1. é¢„è®¡ç®— Corpus Embedding
    print("   è®¡ç®— Corpus Embeddings...")
    corpus_qs = corpus_df['q'].tolist()
    corpus_embs = sbert.encode(corpus_qs, convert_to_tensor=True, show_progress_bar=True, batch_size=128)
    
    # 2. è®¡ç®— Query Embedding
    print("   è®¡ç®— Query Embeddings...")
    q_qs = query_df['q'].tolist()
    q_embs = sbert.encode(q_qs, convert_to_tensor=True, show_progress_bar=True, batch_size=128)
    
    samples = [] # (pos, neg)
    corpus_records = corpus_df.to_dict('records')
    
    eval_data = [] # ç”¨äº MRR è¯„ä¼°: List of {q_id, pos_scores, neg_scores}
    
    for i in tqdm(range(len(query_df)), desc="Building Pairs"):
        q_text = str(q_qs[i])
        gold_ans = str(query_df.iloc[i]['a'])
        q_emb = q_embs[i]
        
        # A. BM25 Retrieve
        scores = bm25.get_scores(q_text.lower().split())
        top_idx = np.argsort(scores)[-args.k_candidates:]
        
        # å‡†å¤‡æ‰¹é‡è®¡ç®— PPL
        candidates = [] # list of dict
        prompts = []
        hashes = []
        
        max_bm25 = max(scores[top_idx]) + 1e-6 # åŠ¨æ€å½’ä¸€åŒ–
        
        # æå–å€™é€‰ä¿¡æ¯
        cand_indices = top_idx.tolist()
        cand_embs_batch = corpus_embs[cand_indices]
        cos_sims = util.cos_sim(q_emb, cand_embs_batch)[0].cpu().numpy()
        
        temp_cands = []
        
        for k, c_idx in enumerate(top_idx):
            c_row = corpus_records[c_idx]
            c_q = str(c_row['q']).strip()
            # [ä¿®å¤ #2] åœ¨ dev æ£€ç´¢æ—¶ï¼Œå¦‚æœç¢°å·§æ£€ç´¢åˆ°è‡ªå·±ï¼ˆè™½ç„¶ç†è®ºä¸Š split äº†ä¸ä¼šï¼Œä½†ä»¥é˜²ä¸‡ä¸€ï¼‰ï¼Œskip
            if normalize_answer(c_q) == normalize_answer(q_text): continue
            
            c_a = str(c_row['a']).strip()
            
            prompt = f"Q: {c_q} A: {c_a}\nQ: {q_text} A:"
            h = get_md5(f"{q_text}_{c_q}_{c_a}")
            
            prompts.append(prompt)
            hashes.append(h)
            
            temp_cands.append({
                'bm25': scores[c_idx] / max_bm25, # Normalized
                'cos': float(cos_sims[k]),
                'ans': c_a
            })
            
        # B. è®¡ç®— PPL
        ppl_vals = ppl_engine.get_ppl_batch(prompts, hashes)
        
        # C. ç»„è£…ç‰¹å¾
        pos_feats = []
        neg_feats = []
        
        for k, cand in enumerate(temp_cands):
            ppl = ppl_vals[k]
            # ç‰¹å¾: [BM25, CosSim, 1/log(PPL)]
            # PPL è¶Šå°è¶Šå¥½ï¼Œæ‰€ä»¥å–å€’æ•°æˆ–è€…è´Ÿå¯¹æ•°è®©å…¶â€œè¶Šå¤§è¶Šå¥½â€
            # log(1) = 0, log(100) = 4.6
            # ä½¿ç”¨ 1 / (1 + log(PPL))ï¼ŒèŒƒå›´ 0-1
            f_ppl = 1.0 / (1.0 + np.log1p(ppl))
            
            feat = [cand['bm25'], cand['cos'], f_ppl]
            
            if is_positive_sample(cand['ans'], gold_ans):
                pos_feats.append(feat)
            else:
                neg_feats.append(feat)
        
        # D. ç”Ÿæˆ Pair
        if pos_feats and neg_feats:
            # è®­ç»ƒé›†ç”Ÿæˆ Pair
            if "train" in name:
                for pf in pos_feats:
                    negs = random.sample(neg_feats, min(len(neg_feats), args.neg_ratio))
                    for nf in negs:
                        samples.append((pf, nf))
            else:
                # éªŒè¯é›†ä¿ç•™å®Œæ•´åˆ—è¡¨ç”¨äºè¯„ä¼° MRR
                # è¿™é‡Œä¸ºäº†ç®€å• Dataset æ ¼å¼ï¼Œè¿˜æ˜¯å­˜ Pairï¼Œä½†åªå­˜ä¸€ä¸ªä»£è¡¨
                samples.append((pos_feats[0], neg_feats[0])) 
    
    print(f"âœ… {name} ç”Ÿæˆäº† {len(samples)} å¯¹æ ·æœ¬")
    torch.save(samples, save_path)
    return samples

# ================= Ranker æ¨¡å‹ =================
class RankerV3(nn.Module):
    def __init__(self):
        super().__init__()
        # 3ç»´ç‰¹å¾: [BM25, CosSim, PPL_Score]
        self.net = nn.Sequential(
            nn.Linear(3, 64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 1)
        )
    def forward(self, x): return self.net(x).squeeze(-1)

# ================= Main =================
def main():
    # 1. åŠ è½½å…¨é‡æ•°æ®å¹¶åˆ‡åˆ†
    full_df = smart_read_csv(args.train_file)
    
    # [ä¿®å¤ #2] ä¸¥æ ¼åˆ‡åˆ†ï¼šæœ€å 10% åš Devï¼Œå‰©ä¸‹åš Train Corpus
    split_idx = int(len(full_df) * 0.9)
    train_corpus = full_df.iloc[:split_idx].reset_index(drop=True) # æ£€ç´¢åº“
    dev_query_df = full_df.iloc[split_idx:].reset_index(drop=True) # éªŒè¯é›†çš„ Query
    
    print(f"ğŸ“Š æ•°æ®é›†åˆ‡åˆ†: Train Corpus={len(train_corpus)}, Dev Query={len(dev_query_df)}")
    
    # 2. å‡†å¤‡èµ„æº
    print("æ„å»º BM25 (åŸºäº Train Corpus)...")
    bm25 = BM25Okapi([str(t).lower().split() for t in train_corpus['q']])
    
    print(f"åŠ è½½ SBERT: {EMBEDDING_MODEL_NAME}...")
    sbert = SentenceTransformer(EMBEDDING_MODEL_NAME, device=device)
    
    print(f"åŠ è½½ GPT-2 PPL å¼•æ“...")
    ppl_engine = PPLEngine(GPT2_MODEL_NAME, DB_PATH)
    
    # 3. ç”Ÿæˆæ•°æ® (æ³¨æ„ï¼šDev ä¹Ÿæ˜¯åœ¨ Train Corpus é‡Œæœ)
    train_data = build_dataset(train_corpus, train_corpus, bm25, sbert, ppl_engine, "train")
    dev_data = build_dataset(dev_query_df, train_corpus, bm25, sbert, ppl_engine, "dev")
    
    ppl_engine.close()
    
    # 4. è®­ç»ƒ
    class MyDS(Dataset):
        def __init__(self, d): self.d = d
        def __len__(self): return len(self.d)
        def __getitem__(self, i): 
            return torch.tensor(self.d[i][0], dtype=torch.float), torch.tensor(self.d[i][1], dtype=torch.float)
            
    train_loader = DataLoader(MyDS(train_data), batch_size=args.batch_size, shuffle=True)
    val_loader = DataLoader(MyDS(dev_data), batch_size=args.batch_size, shuffle=False)
    
    ranker = RankerV3().to(device)
    optimizer = optim.Adam(ranker.parameters(), lr=args.lr)
    criterion = nn.MarginRankingLoss(margin=0.1)
    
    print("ğŸ”¥ å¼€å§‹è®­ç»ƒ V3 Ranker...")
    for epoch in range(args.epochs):
        ranker.train()
        total_loss = 0
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}")
        for pos, neg in pbar:
            pos, neg = pos.to(device), neg.to(device)
            optimizer.zero_grad()
            sp, sn = ranker(pos), ranker(neg)
            loss = criterion(sp, sn, torch.ones_like(sp))
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            pbar.set_postfix({'loss': f"{loss.item():.4f}"})
            
        # ç®€å•éªŒè¯
        ranker.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for pos, neg in val_loader:
                pos, neg = pos.to(device), neg.to(device)
                correct += (ranker(pos) > ranker(neg)).sum().item()
                total += pos.size(0)
        print(f"Epoch {epoch+1} Val Pair Acc: {correct/total:.4f}")
        
    torch.save(ranker.state_dict(), "ranker_v3.pth")
    print("âœ… æ¨¡å‹ä¿å­˜ä¸º ranker_v3.pth")

if __name__ == "__main__":
    main()
