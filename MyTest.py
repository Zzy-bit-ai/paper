import os
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import requests
import time
import string
import json
import argparse
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM
from rank_bm25 import BM25Okapi

# ==================== å‚æ•°é…ç½® ====================
parser = argparse.ArgumentParser(description="TriviaQA Retrieve + Rerank + Few-shot Generate (Fixed Version)")
parser.add_argument('--train_file', type=str, required=True, help="è®­ç»ƒé›† TSVï¼ˆä½œä¸ºè¯­æ–™åº“ï¼‰")
parser.add_argument('--dev_file', type=str, required=True, help="éªŒè¯é›† TSVï¼ˆæµ‹è¯•é—®é¢˜ï¼‰")
parser.add_argument('--ranker_model', type=str, required=True, help="è®­ç»ƒå¥½çš„ ranker pth æƒé‡è·¯å¾„")
parser.add_argument('--hf_model_path', type=str, default='gpt2', help="ç”¨äºè®¡ç®— PPL çš„æ¨¡å‹")
parser.add_argument('--llm_model', type=str, default='qwen:1.8b', help="Ollama æ¨¡å‹åç§°ï¼Œä¾‹å¦‚ qwen:1.8b")
parser.add_argument('--rough_k', type=int, default=100, help="BM25 ç²—æ’å€™é€‰æ•°é‡ï¼ˆæ¨è 100+ï¼‰")
parser.add_argument('--k_candidates', type=int, default=8, help="æœ€ç»ˆé€‰ä¸º Few-shot ç¤ºä¾‹çš„æ•°é‡")
parser.add_argument('--max_samples', type=int, default=None, help="æµ‹è¯•æ ·æœ¬æ•°é‡ï¼ŒNone=å…¨éƒ¨")
parser.add_argument('--use_ranker', action='store_true', default=True, help="æ˜¯å¦ä½¿ç”¨ rankerï¼ˆå…³é—­å³ä¸ºçº¯ BM25 åŸºçº¿ï¼‰")
parser.add_argument('--no_use_ranker', dest='use_ranker', action='store_false', help="å…³é—­ rankerï¼Œä½¿ç”¨ BM25 åŸºçº¿")
parser.add_argument('--output_json', type=str, default='qa_results.json', help="è¯¦ç»†ç»“æœä¿å­˜è·¯å¾„")
parser.add_argument('--ollama_retries', type=int, default=3, help="Ollama è°ƒç”¨é‡è¯•æ¬¡æ•°")
parser.add_argument('--ollama_timeout', type=int, default=60, help="Ollama è¯·æ±‚è¶…æ—¶æ—¶é—´")
args = parser.parse_args()

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# ==================== ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´çš„æ¨¡å‹ç»“æ„ ====================
class LocalRanker(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(2, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.1),        # è®­ç»ƒæ—¶æ˜¯ 0.1
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )

    def forward(self, x):
        return self.net(x).squeeze(-1)

def load_ranker(path):
    model = LocalRanker().to(device)
    # ä½¿ç”¨ weights_only=True é¿å…å®‰å…¨è­¦å‘Šå¹¶æé«˜å®‰å…¨æ€§
    model.load_state_dict(torch.load(path, map_location=device, weights_only=True))
    model.eval()
    return model

# ==================== è¾…åŠ©å·¥å…· ====================
def normalize_answer(s):
    s = str(s).lower()
    s = ''.join(ch for ch in s if ch not in set(string.punctuation))
    return ' '.join(s.split())

def exact_match(pred, gold):
    return normalize_answer(pred) == normalize_answer(gold)

# ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´çš„ç‰¹å¾ç¼©æ”¾
def scale_features(ppl_val: float, bm25_score: float):
    return [np.log1p(ppl_val) / 5.0, bm25_score / 20.0]

# ==================== PPL è®¡ç®—ï¼ˆæ‰¹å¤„ç†ï¼‰ ====================
print("ğŸ¤– åŠ è½½ PPL è®¡ç®—æ¨¡å‹...")
tokenizer = AutoTokenizer.from_pretrained(args.hf_model_path)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

ppl_model = AutoModelForCausalLM.from_pretrained(
    args.hf_model_path,
    torch_dtype=torch.float16
).to(device)
ppl_model.eval()

def compute_ppl_batch(prompts):
    if not prompts:
        return np.array([])
    inputs = tokenizer(
        prompts,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=512
    ).to(device)

    with torch.no_grad():
        outputs = ppl_model(**inputs, labels=inputs["input_ids"])

    shift_logits = outputs.logits[..., :-1, :].contiguous()
    shift_labels = inputs["input_ids"][..., 1:].contiguous()

    loss_fct = nn.CrossEntropyLoss(reduction='none')
    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
    loss = loss.view(shift_labels.shape)

    mask = (shift_labels != tokenizer.pad_token_id).float()
    seq_loss = (loss * mask).sum(dim=1) / torch.clamp(mask.sum(dim=1), min=1.0)
    return torch.exp(seq_loss).cpu().numpy()

# ==================== Ollama æ¨ç†ï¼ˆå¸¦é‡è¯•ï¼‰ ====================
def ollama_generate(prompt: str):
    url = "http://localhost:11434/api/generate"
    payload = {
        "model": args.llm_model,
        "prompt": prompt,
        "stream": False,
        "temperature": 0.0,
        "options": {
            "num_predict": 64,
            "stop": ["\n", "\n\n", "Question:", "question:"]
        }
    }

    for attempt in range(args.ollama_retries):
        try:
            r = requests.post(url, json=payload, timeout=args.ollama_timeout)
            if r.status_code == 200:
                response = r.json().get("response", "").strip()
                if response:
                    return response
        except Exception as e:
            print(f"  âš ï¸ Ollama è°ƒç”¨å¤±è´¥ (å°è¯• {attempt+1}/{args.ollama_retries}): {e}")
            time.sleep(2 ** attempt)
    return ""

# ==================== ä¸»æµç¨‹ ====================
def main():
    ranker = load_ranker(args.ranker_model) if args.use_ranker else None

    print("ğŸ“‚ åŠ è½½æ•°æ®...")
    train_df = pd.read_csv(args.train_file, sep='\t', quoting=3, on_bad_lines='skip').fillna("")
    dev_df   = pd.read_csv(args.dev_file,   sep='\t', quoting=3, on_bad_lines='skip').fillna("")

    test_df = dev_df.head(args.max_samples) if args.max_samples else dev_df
    print(f"æµ‹è¯•æ ·æœ¬æ•°: {len(test_df)}")

    corpus_records = train_df.to_dict('records')
    print("ğŸ” æ„å»º BM25 ç´¢å¼•...")
    bm25 = BM25Okapi([str(r['q']).lower().split() for r in corpus_records])

    results = []
    correct_count = 0

    for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc="QA å¤„ç†"):
        q_test = str(row['q']).strip()
        gold   = str(row['a']).strip()

        # 1. BM25 ç²—æ’ Top-rough_k
        bm25_scores = bm25.get_scores(q_test.lower().split())
        top_indices = np.argsort(bm25_scores)[-args.rough_k:]

        cands = [corpus_records[i] for i in top_indices]
        bm25_subscores = [bm25_scores[i] for i in top_indices]

        # 2. è®¡ç®— PPL
        prompts = [f"Q: {c['q']} A: {c['a']}\nQ: {q_test} A:" for c in cands]
        ppls = compute_ppl_batch(prompts)

        # 3. æ„é€ ç‰¹å¾
        features = [scale_features(ppls[i], bm25_subscores[i]) for i in range(len(cands))]
        features_tensor = torch.tensor(features, dtype=torch.float).to(device)

        # 4. é‡æ’åº
        if args.use_ranker and ranker is not None:
            with torch.no_grad():
                rank_scores = ranker(features_tensor).cpu().numpy()
            sorted_indices = np.argsort(rank_scores)[::-1]  # ä»é«˜åˆ°ä½
        else:
            # åŸºçº¿ï¼šç›´æ¥æŒ‰ BM25 åˆ†æ•°ï¼ˆå·²é™åºï¼‰
            sorted_indices = list(range(len(cands)))

        # 5. é€‰å– Top-k ä½œä¸º Few-shot ç¤ºä¾‹
        selected_indices = sorted_indices[:args.k_candidates]
        selected_cands = [cands[i] for i in selected_indices]

        # 6. æ„é€ æ¸…æ™°çš„ Prompt
        final_prompt = (
            "Use the following examples to answer the question. "
            "Provide only the answer, no explanation.\n\n"
        )
        for c in selected_cands:
            final_prompt += f"Question: {c['q']}\nAnswer: {c['a']}\n\n"
        final_prompt += f"Question: {q_test}\nAnswer:"

        # 7. è°ƒç”¨ Ollama ç”Ÿæˆç­”æ¡ˆ
        pred = ollama_generate(final_prompt)

        is_correct = exact_match(pred, gold)
        if is_correct:
            correct_count += 1

        results.append({
            "question": q_test,
            "prediction": pred,
            "gold": gold,
            "correct": is_correct,
            "method": "ranker" if args.use_ranker else "bm25_baseline"
        })

    # ==================== ç»“æœè¾“å‡º ====================
    accuracy = correct_count / len(results)
    print(f"\nâœ… æµ‹è¯•å®Œæˆï¼")
    print(f"   æ–¹æ³•: {'Ranker + Few-shot' if args.use_ranker else 'BM25 åŸºçº¿ Few-shot'}")
    print(f"   Exact Match å‡†ç¡®ç‡: {accuracy:.4f} ({correct_count}/{len(results)})")

    # ä¿å­˜è¯¦ç»†ç»“æœ
    with open(args.output_json, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"   è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {args.output_json}")

if __name__ == "__main__":
    main()